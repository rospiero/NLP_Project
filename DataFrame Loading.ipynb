{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9a6b785c",
   "metadata": {},
   "source": [
    "The function of this notebook is to load, clean and embed the dataframe to be used in the final model. The dataframe is then saved in a .csv file, which can be loaded in the final program. This achieves optimization in the final program and website deployment. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce91ee95",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import kagglehub\n",
    "import pandas as pd\n",
    "import os\n",
    "import pickle\n",
    "from pathlib import Path\n",
    "import joblib\n",
    "import re\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")  # Suppress warnings for cleaner output\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from dotenv import load_dotenv\n",
    "import openai\n",
    "from datetime import datetime\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e790027",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "# Import all necessary libraries for this program\n",
    "\n",
    "\n",
    "\n",
    "# Download dataset\n",
    "path = kagglehub.dataset_download(\"datafiniti/consumer-reviews-of-amazon-products\")\n",
    "\n",
    "# Read CSV files\n",
    "file_path1 = os.path.join(path, \"1429_1.csv\")\n",
    "file_path2 = os.path.join(path, \"Datafiniti_Amazon_Consumer_Reviews_of_Amazon_Products.csv\")\n",
    "file_path3 = os.path.join(path, \"Datafiniti_Amazon_Consumer_Reviews_of_Amazon_Products_May19.csv\")\n",
    "df1 = pd.read_csv(file_path1)\n",
    "df2 = pd.read_csv(file_path2)\n",
    "df3 = pd.read_csv(file_path3)\n",
    "\n",
    "# Load pickle mapping\n",
    "\n",
    "pickle_file_path = Path.cwd() / \"Joblib_files\" / \"unique_categories_dict.pkl\"\n",
    "if not pickle_file_path.exists():\n",
    "    raise FileNotFoundError(f\"Missing pickle file: {pickle_file_path}\")\n",
    "with open(pickle_file_path, \"rb\") as f:\n",
    "    meta_category_mapping = pickle.load(f)\n",
    "\n",
    "# Filter and prepare data\n",
    "columns_df1 = ['name', 'asins', 'categories', 'reviews.doRecommend', 'reviews.numHelpful', 'reviews.rating', 'reviews.text', 'reviews.title']\n",
    "columns_other = columns_df1 + ['imageURLs']\n",
    "df1_filtered = df1[columns_df1]\n",
    "df1_filtered['imageURLs'] = \"https://upload.wikimedia.org/wikipedia/commons/a/ac/No_image_available.svg\"\n",
    "df2_filtered = df2[columns_other]\n",
    "df3_filtered = df3[columns_other]\n",
    "\n",
    "# Combine the dataframes, and map the meta-categories that we extracted via the clustering model to the 'categories' column\n",
    "df_combined = pd.concat([df1_filtered, df2_filtered, df3_filtered], ignore_index=True)\n",
    "df_combined['meta_category'] = df_combined['categories'].map(meta_category_mapping).fillna(\"Unknown\")\n",
    "\n",
    "# Print output of the combined dataframe\n",
    "print(\"Shape of the combined dataframe:\", df_combined.shape)\n",
    "print(df_combined.head())\n",
    "\n",
    "\n",
    "# First we need to clean the data with our data cleaning function, taking into account that the reviews.text column is a string but might contain floats\n",
    "# Clean the review text column and append the cleaned text to a new column, given the size of the dataset this will not increase the model runtime significantly\n",
    "def light_clean(text):\n",
    "    if isinstance(text, float):\n",
    "        text = str(text)  # Convert float to string\n",
    "    text = text.strip()\n",
    "    text = re.sub(r\"\\s+\", \" \", text)  # normalize whitespace\n",
    "    return text\n",
    "\n",
    "df_combined[\"cleaned_text\"] = df_combined[\"reviews.text\"].apply(light_clean)\n",
    "\n",
    "# Then we convert the relevant columns of the dataframe to word embeddings using a pre-trained sentence transformer model. We use a sentence transformer as these are designed \n",
    "# to work well with natural, unaltered sentences and are trained on a large corpus of text data, making them suitable for generating embeddings for a wide range of text inputs.\n",
    "\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "# Generate embeddings for the cleaned review text\n",
    "embeddings = model.encode(df_combined[\"cleaned_text\"].tolist(), show_progress_bar=True, convert_to_tensor= True, device='cuda')\n",
    "# Append the embeddings to the dataframe\n",
    "df_combined[\"embeddings\"] = embeddings.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9597e9d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the combined dataframe with embeddings to a CSV file\n",
    "output_file_path = Path.cwd() / \"Joblib_files\" / \"amazon_reviews_with_embeddings.csv\"\n",
    "df_combined.to_csv(output_file_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "210d259d",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()  # Load environment variables from .env file\n",
    "\n",
    "# Use the standard environment variable name for OpenAI API key\n",
    "api_key = os.getenv(\"OPENAI_TEST_KEY_KdR\")\n",
    "if not api_key:\n",
    "    raise ValueError(\"OpenAI API key not found. Please set the OPENAI_TEST_KEY_KdR environment variable.\")\n",
    "\n",
    "client = openai.OpenAI(api_key=api_key)\n",
    "\n",
    "# Print the OpenAI API key to confirm it is loaded correctly (for debugging, remove in production)\n",
    "print(\"OpenAI API key loaded successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "296d9d48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine all the dataframes that contain the final model output into a single dataframe, that can be loaded into the website\n",
    "files_to_load = [\"Batteries_Summary.csv\", \"Connected_Home_Electronics.csv\", \"Kitchen_Storage_Summary.csv\", \n",
    "                  \"Office_Supplies_Summary.csv\", \"Pet_Products_Summary.csv\", \"Portable_Electronics.csv\"]\n",
    "\n",
    "# Load each file to load and concatenate them into a single dataframe\n",
    "dataframes = []\n",
    "for file_name in files_to_load:\n",
    "    file_path = Path.cwd() / \"Product Summaries\" / file_name\n",
    "    if not file_path.exists():\n",
    "        print(f\"File {file_name} does not exist, skipping.\")\n",
    "        continue\n",
    "    df = pd.read_csv(file_path)\n",
    "    dataframes.append(df)\n",
    "\n",
    "# Concatenate all dataframes into a single dataframe\n",
    "Summaries_df = pd.concat(dataframes, ignore_index=True)\n",
    "# Save the combined dataframe to a CSV file\n",
    "output_summaries_path = Path.cwd() / \"Product Summaries\" / \"Summaries Combined.csv\"\n",
    "Summaries_df.to_csv(output_summaries_path, index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "776b321e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_best_image_url(url_string):\n",
    "    \"\"\"\n",
    "    Extract the best valid image URL from a comma-separated list.\n",
    "    Decodes URL-encoded parts, filters for known image hosts.\n",
    "    \"\"\"\n",
    "    placeholder = \"https://upload.wikimedia.org/wikipedia/commons/a/ac/No_image_available.svg\"\n",
    "    \n",
    "    if pd.isna(url_string) or not url_string.strip():\n",
    "        return placeholder\n",
    "\n",
    "    urls = [url.strip() for url in url_string.split(\",\") if url.strip()]\n",
    "    \n",
    "    trusted_domains = [\n",
    "        'amazon.com',\n",
    "        'ebayimg.com'\n",
    "    ]\n",
    "\n",
    "    for url in urls:\n",
    "        decoded_url = urllib.parse.unquote(url)\n",
    "        for domain in trusted_domains:\n",
    "            if domain in decoded_url:\n",
    "                return decoded_url  # return first valid match\n",
    "\n",
    "    # fallback if none matched\n",
    "    return placeholder"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
